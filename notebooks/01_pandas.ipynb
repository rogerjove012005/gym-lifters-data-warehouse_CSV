{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# FASE 3: PROCESO ETL CON PANDAS\n",
    "# ============================================================\n",
    "# En esta fase vamos a:\n",
    "# 1. Extraer: Leer el dataset limpio\n",
    "# 2. Transformar: Crear tablas dimensionales y de hechos\n",
    "# 3. Cargar: Guardar en SQLite (warehouse_pandas.db)\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 1: EXTRACCIÃ“N (E) - Cargar dataset limpio\n",
    "# ============================================================\n",
    "# Cargamos el dataset que limpiamos anteriormente\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PASO 1: EXTRACCIÃ“N (E)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar el dataset limpio\n",
    "data_path = \"data/gym_lifters_clean.csv\"\n",
    "df_clean = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"âœ“ Dataset limpio cargado: {len(df_clean)} filas\")\n",
    "print(f\"âœ“ Columnas: {len(df_clean.columns)}\")\n",
    "print(\"\\nPrimeras 3 filas:\")\n",
    "display(df_clean.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 2: TRANSFORMACIÃ“N (T) - Crear Modelo Dimensional\n",
    "\n",
    "Vamos a crear:\n",
    "- **dim_athlete**: InformaciÃ³n Ãºnica de cada atleta\n",
    "- **dim_competition**: InformaciÃ³n de las competencias  \n",
    "- **dim_team**: InformaciÃ³n de equipos y coaches\n",
    "- **fact_lifting**: Tabla de hechos con mÃ©tricas de levantamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 2: TRANSFORMACIÃ“N (T)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PASO 2: TRANSFORMACIÃ“N (T)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# DIMENSIÃ“N 1: dim_athlete\n",
    "# ============================================================\n",
    "print(\"\\n2.1 Creando dim_athlete...\")\n",
    "\n",
    "# Seleccionar columnas de atleta y eliminar duplicados\n",
    "dim_athlete = df_clean[['athlete_id', 'name', 'gender', 'age', 'country']].copy()\n",
    "\n",
    "# Filtrar filas con nombre vÃ¡lido\n",
    "dim_athlete = dim_athlete[dim_athlete['name'].notna() & (dim_athlete['name'] != '')]\n",
    "\n",
    "# Eliminar duplicados (mantener el primero)\n",
    "dim_athlete = dim_athlete.drop_duplicates(subset=['name', 'country'], keep='first')\n",
    "\n",
    "# Crear ID numÃ©rico Ãºnico para cada atleta\n",
    "dim_athlete = dim_athlete.reset_index(drop=True)\n",
    "dim_athlete.insert(0, 'id_athlete', range(1, len(dim_athlete) + 1))\n",
    "\n",
    "# Limpiar athlete_id: si estÃ¡ vacÃ­o, generar uno nuevo\n",
    "dim_athlete['athlete_id'] = dim_athlete.apply(\n",
    "    lambda row: row['athlete_id'] if pd.notna(row['athlete_id']) and row['athlete_id'] != '' \n",
    "    else f\"ath_{row['id_athlete']}\", axis=1\n",
    ")\n",
    "\n",
    "# Seleccionar columnas finales en el orden correcto\n",
    "dim_athlete = dim_athlete[['id_athlete', 'athlete_id', 'name', 'gender', 'age', 'country']]\n",
    "\n",
    "print(f\"   âœ“ dim_athlete creada: {len(dim_athlete)} atletas Ãºnicos\")\n",
    "print(f\"   âœ“ Columnas: {list(dim_athlete.columns)}\")\n",
    "display(dim_athlete.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DIMENSIÃ“N 2: dim_competition\n",
    "# ============================================================\n",
    "print(\"\\n2.2 Creando dim_competition...\")\n",
    "\n",
    "# Seleccionar columnas de competencia y eliminar duplicados\n",
    "dim_competition = df_clean[['competition', 'year', 'category']].copy()\n",
    "\n",
    "# Filtrar filas con competencia y aÃ±o vÃ¡lidos\n",
    "dim_competition = dim_competition[\n",
    "    dim_competition['competition'].notna() & \n",
    "    dim_competition['year'].notna()\n",
    "]\n",
    "\n",
    "# Eliminar duplicados\n",
    "dim_competition = dim_competition.drop_duplicates(subset=['competition', 'year', 'category'], keep='first')\n",
    "\n",
    "# Crear ID numÃ©rico Ãºnico para cada competencia\n",
    "dim_competition = dim_competition.reset_index(drop=True)\n",
    "dim_competition.insert(0, 'id_competition', range(1, len(dim_competition) + 1))\n",
    "\n",
    "# Seleccionar columnas finales\n",
    "dim_competition = dim_competition[['id_competition', 'competition', 'year', 'category']]\n",
    "\n",
    "print(f\"   âœ“ dim_competition creada: {len(dim_competition)} competencias Ãºnicas\")\n",
    "print(f\"   âœ“ Columnas: {list(dim_competition.columns)}\")\n",
    "display(dim_competition.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DIMENSIÃ“N 3: dim_team\n",
    "# ============================================================\n",
    "print(\"\\n2.3 Creando dim_team...\")\n",
    "\n",
    "# Seleccionar columnas de equipo y eliminar duplicados\n",
    "dim_team = df_clean[['team', 'coach']].copy()\n",
    "\n",
    "# Filtrar filas con team vÃ¡lido\n",
    "dim_team = dim_team[dim_team['team'].notna() & (dim_team['team'] != '')]\n",
    "\n",
    "# Eliminar duplicados\n",
    "dim_team = dim_team.drop_duplicates(subset=['team', 'coach'], keep='first')\n",
    "\n",
    "# Crear ID numÃ©rico Ãºnico para cada equipo\n",
    "dim_team = dim_team.reset_index(drop=True)\n",
    "dim_team.insert(0, 'id_team', range(1, len(dim_team) + 1))\n",
    "\n",
    "# Seleccionar columnas finales\n",
    "dim_team = dim_team[['id_team', 'team', 'coach']]\n",
    "\n",
    "print(f\"   âœ“ dim_team creada: {len(dim_team)} equipos Ãºnicos\")\n",
    "print(f\"   âœ“ Columnas: {list(dim_team.columns)}\")\n",
    "display(dim_team.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TABLA DE HECHOS: fact_lifting\n",
    "# ============================================================\n",
    "# La tabla de hechos contiene las mÃ©tricas de levantamiento\n",
    "# y se relaciona con las 3 dimensiones mediante claves forÃ¡neas\n",
    "print(\"\\n2.4 Creando fact_lifting (Tabla de Hechos)...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Preparar el DataFrame base con todos los datos limpios\n",
    "fact_base = df_clean.copy()\n",
    "print(f\"   Registros originales: {len(fact_base)}\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 1: Relacionar con dim_athlete\n",
    "# ============================================================\n",
    "# Usamos merge (equivalente a JOIN en SQL) para relacionar\n",
    "# los datos con la dimensiÃ³n de atletas\n",
    "print(\"\\n   Paso 1: Relacionando con dim_athlete...\")\n",
    "fact_with_athlete = fact_base.merge(\n",
    "    dim_athlete[['id_athlete', 'name', 'country']],  # Solo las columnas necesarias\n",
    "    on=['name', 'country'],  # Clave de uniÃ³n: nombre y paÃ­s\n",
    "    how='left'  # Left join: mantiene todos los registros de fact_base\n",
    ")\n",
    "\n",
    "# Contar cuÃ¡ntos registros encontraron match\n",
    "matches_athlete = fact_with_athlete['id_athlete'].notna().sum()\n",
    "print(f\"      Registros con atleta encontrado: {matches_athlete} ({matches_athlete/len(fact_base)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 2: Relacionar con dim_competition\n",
    "# ============================================================\n",
    "print(\"\\n   Paso 2: Relacionando con dim_competition...\")\n",
    "fact_with_competition = fact_with_athlete.merge(\n",
    "    dim_competition[['id_competition', 'competition', 'year', 'category']],\n",
    "    on=['competition', 'year', 'category'],  # Clave de uniÃ³n: competencia, aÃ±o y categorÃ­a\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "matches_competition = fact_with_competition['id_competition'].notna().sum()\n",
    "print(f\"      Registros con competencia encontrada: {matches_competition} ({matches_competition/len(fact_base)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 3: Relacionar con dim_team\n",
    "# ============================================================\n",
    "print(\"\\n   Paso 3: Relacionando con dim_team...\")\n",
    "fact_with_team = fact_with_competition.merge(\n",
    "    dim_team[['id_team', 'team', 'coach']],\n",
    "    on=['team', 'coach'],  # Clave de uniÃ³n: equipo y coach\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "matches_team = fact_with_team['id_team'].notna().sum()\n",
    "print(f\"      Registros con equipo encontrado: {matches_team} ({matches_team/len(fact_base)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 4: Filtrar registros vÃ¡lidos\n",
    "# ============================================================\n",
    "# Solo mantenemos los registros que tienen todas las claves forÃ¡neas\n",
    "# (id_athlete, id_competition, id_team) vÃ¡lidas\n",
    "print(\"\\n   Paso 4: Filtrando registros vÃ¡lidos...\")\n",
    "fact_lifting = fact_with_team[\n",
    "    fact_with_team['id_athlete'].notna() &      # Debe tener atleta\n",
    "    fact_with_team['id_competition'].notna() &  # Debe tener competencia\n",
    "    fact_with_team['id_team'].notna()           # Debe tener equipo\n",
    "].copy()\n",
    "\n",
    "print(f\"      Registros vÃ¡lidos (con todas las relaciones): {len(fact_lifting)}\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 5: Seleccionar columnas finales\n",
    "# ============================================================\n",
    "# La tabla de hechos contiene:\n",
    "# - Claves forÃ¡neas (id_athlete, id_competition, id_team)\n",
    "# - MÃ©tricas de levantamiento (pesos, rankings, medallas, etc.)\n",
    "print(\"\\n   Paso 5: Seleccionando columnas finales...\")\n",
    "fact_lifting = fact_lifting[[\n",
    "    # Claves forÃ¡neas (relaciones con dimensiones)\n",
    "    'id_athlete',        # RelaciÃ³n con dim_athlete\n",
    "    'id_competition',    # RelaciÃ³n con dim_competition\n",
    "    'id_team',           # RelaciÃ³n con dim_team\n",
    "    \n",
    "    # MÃ©tricas de levantamiento\n",
    "    'snatch_kg',         # Peso levantado en snatch (kg)\n",
    "    'clean_and_jerk_kg', # Peso levantado en clean and jerk (kg)\n",
    "    'total_kg',          # Total levantado (kg)\n",
    "    'body_weight_kg',    # Peso corporal del atleta (kg)\n",
    "    'event_rank',        # PosiciÃ³n en la competencia\n",
    "    \n",
    "    # InformaciÃ³n adicional\n",
    "    'medal',             # Medalla obtenida (Gold, Silver, Bronze)\n",
    "    'record_status',     # Estado del rÃ©cord (World Record, National Record, etc.)\n",
    "    'lifting_style'      # Estilo de levantamiento (Olympic, Powerlifting, etc.)\n",
    "]]\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN DE fact_lifting:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   âœ“ Total de registros: {len(fact_lifting)}\")\n",
    "print(f\"   âœ“ Columnas: {len(fact_lifting.columns)}\")\n",
    "print(f\"   âœ“ Registros originales: {len(fact_base)}\")\n",
    "print(f\"   âœ“ Porcentaje procesado: {len(fact_lifting)/len(fact_base)*100:.1f}%\")\n",
    "print(f\"   âœ“ Columnas en fact_lifting: {list(fact_lifting.columns)}\")\n",
    "\n",
    "print(\"\\n   Muestra de los primeros 5 registros:\")\n",
    "print(\"-\" * 60)\n",
    "display(fact_lifting.head(5))\n",
    "\n",
    "print(\"\\n   InformaciÃ³n de la tabla:\")\n",
    "print(\"-\" * 60)\n",
    "print(fact_lifting.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 3: CARGA (L) - Guardar en SQLite\n",
    "\n",
    "Cargamos todas las tablas en la base de datos SQLite `warehouse_pandas.db`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 3: CARGA (L) - Guardar en SQLite\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PASO 3: CARGA (L)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear directorio warehouse si no existe\n",
    "import os\n",
    "os.makedirs(\"warehouse\", exist_ok=True)\n",
    "\n",
    "# Ruta de la base de datos\n",
    "db_path = \"warehouse/warehouse_pandas.db\"\n",
    "\n",
    "# Eliminar base de datos existente si existe (para recrearla)\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(\"âš ï¸ Base de datos existente eliminada\")\n",
    "\n",
    "# Crear conexiÃ³n a SQLite usando SQLAlchemy\n",
    "engine = create_engine(f'sqlite:///{db_path}', echo=False)\n",
    "print(f\"âœ“ ConexiÃ³n a SQLite establecida: {db_path}\")\n",
    "\n",
    "# Cargar tablas en SQLite usando to_sql()\n",
    "print(\"\\nğŸ“Š Cargando tablas en SQLite...\")\n",
    "\n",
    "# Cargar dim_athlete\n",
    "dim_athlete.to_sql(\"dim_athlete\", engine, if_exists=\"replace\", index=False)\n",
    "print(f\"   âœ“ dim_athlete: {len(dim_athlete)} registros cargados\")\n",
    "\n",
    "# Cargar dim_competition\n",
    "dim_competition.to_sql(\"dim_competition\", engine, if_exists=\"replace\", index=False)\n",
    "print(f\"   âœ“ dim_competition: {len(dim_competition)} registros cargados\")\n",
    "\n",
    "# Cargar dim_team\n",
    "dim_team.to_sql(\"dim_team\", engine, if_exists=\"replace\", index=False)\n",
    "print(f\"   âœ“ dim_team: {len(dim_team)} registros cargados\")\n",
    "\n",
    "# Cargar fact_lifting\n",
    "fact_lifting.to_sql(\"fact_lifting\", engine, if_exists=\"replace\", index=False)\n",
    "print(f\"   âœ“ fact_lifting: {len(fact_lifting)} registros cargados\")\n",
    "\n",
    "print(f\"\\nâœ… Proceso ETL completado. Base de datos guardada en: {db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICACIÃ“N DE DATOS CARGADOS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICACIÃ“N DE DATOS CARGADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar datos cargados\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Consultar nÃºmero de registros en cada tabla\n",
    "tables = [\"dim_athlete\", \"dim_competition\", \"dim_team\", \"fact_lifting\"]\n",
    "print(\"\\nğŸ“Š Resumen de tablas en warehouse_pandas.db:\\n\")\n",
    "\n",
    "for table in tables:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"   {table}: {count} registros\")\n",
    "\n",
    "# Mostrar muestras de cada tabla\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MUESTRA DE dim_athlete:\")\n",
    "print(\"-\" * 60)\n",
    "pd.read_sql_query(\"SELECT * FROM dim_athlete LIMIT 5\", conn)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MUESTRA DE dim_competition:\")\n",
    "print(\"-\" * 60)\n",
    "pd.read_sql_query(\"SELECT * FROM dim_competition LIMIT 5\", conn)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MUESTRA DE dim_team:\")\n",
    "print(\"-\" * 60)\n",
    "pd.read_sql_query(\"SELECT * FROM dim_team LIMIT 5\", conn)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MUESTRA DE fact_lifting:\")\n",
    "print(\"-\" * 60)\n",
    "pd.read_sql_query(\"SELECT * FROM fact_lifting LIMIT 5\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Â¡ETL CON PANDAS COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gym_lifters.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Cargamos los datos del csv con el read\u001b[39;00m\n\u001b[32m      7\u001b[39m data_path = \u001b[33m\"\u001b[39m\u001b[33mgym_lifters.csv\u001b[39m\u001b[33m\"\u001b[39m  \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset cargado correctamente\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFilas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Columnas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'gym_lifters.csv'"
     ]
    }
   ],
   "source": [
    "# PROYECTO RA1 - FASE 1: ExploraciÃ³n y Limpieza con Pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos los datos del csv con el read\n",
    "data_path = \"data/gym_lifters.csv\"  \n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "print(\"Dataset cargado correctamente\")\n",
    "print(f\"Filas: {df.shape[0]} | Columnas: {df.shape[1]}\")\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "\n",
    "print(\"InformaciÃ³n general del DataFrame:\\n\")\n",
    "df.head()\n",
    "df.info()\n",
    "\n",
    "print(\"Tipos de datos por columna:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"DescripciÃ³n estadÃ­stica de columnas numÃ©ricas:\")\n",
    "display(df.describe())\n",
    "\n",
    "\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"Filas duplicadas:\", df.duplicated().sum())\n",
    "\n",
    "# Se eliminan los duplicados si existen\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Estandarizar nombres de columnas a snake_case\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Eliminar espacios en texto y uniformizar mayÃºsculas/minÃºsculas\n",
    "for col in df.select_dtypes(include=\"object\").columns:\n",
    "    df[col] = df[col].str.strip().str.lower()\n",
    "\n",
    "# Limpiar valores problemÃ¡ticos antes de convertir a numÃ©rico\n",
    "# Reemplazar valores invÃ¡lidos comunes por NaN\n",
    "valores_invalidos = ['?', 'n/a', 'na', 'unknown', '-1', 'none']\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].replace(valores_invalidos, np.nan)\n",
    "\n",
    "# Limpiar unidades de texto en columnas numÃ©ricas (ej: \"154kg\" -> 154)\n",
    "columnas_peso = ['snatch_kg', 'clean_and_jerk_kg', 'total_kg', 'body_weight_kg']\n",
    "for col in columnas_peso:\n",
    "    if col in df.columns:\n",
    "        # Extraer solo nÃºmeros, manejar casos como \"154kg\", \"300kg\", etc.\n",
    "        df[col] = df[col].astype(str).str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convertir columnas numÃ©ricas a tipos apropiados\n",
    "# Age\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors='coerce')\n",
    "\n",
    "# Year (aÃ±o)\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors='coerce')\n",
    "# Limpiar aÃ±os invÃ¡lidos (ej: 9999, aÃ±os futuros)\n",
    "df[\"year\"] = df[\"year\"].where((df[\"year\"] >= 1900) & (df[\"year\"] <= 2025), np.nan)\n",
    "\n",
    "# Event rank\n",
    "df[\"event_rank\"] = pd.to_numeric(df[\"event_rank\"], errors='coerce')\n",
    "\n",
    "# Tratar valores faltantes en columnas numÃ©ricas\n",
    "# Age: rellenar con mediana\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Pesos: rellenar con mediana de cada columna\n",
    "for col in columnas_peso:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Year: rellenar con moda (aÃ±o mÃ¡s comÃºn)\n",
    "df[\"year\"] = df[\"year\"].fillna(df[\"year\"].mode()[0] if not df[\"year\"].mode().empty else np.nan)\n",
    "\n",
    "# Convertir columnas a tipos de datos adecuados\n",
    "print(\"\\n[OK] Dataset limpio:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n[INFO] Tipos de datos finales:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n[INFO] Valores nulos despuÃ©s de limpieza:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\n[INFO] Resumen estadÃ­stico de columnas numÃ©ricas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Vista final de los datos\n",
    "display(df.sample(5))\n",
    "\n",
    "# Guardar el DataFrame limpio en un nuevo archivo CSV\n",
    "clean_path = \"data/gym_lifters_clean.csv\"\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Dataset limpio guardado en: {clean_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
