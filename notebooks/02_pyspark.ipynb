{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PROYECTO RA1 - FASE 2: Procesamiento con PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import os\n",
        "\n",
        "# Crear SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GymLiftersETL\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"warehouse\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurar nivel de logging para reducir verbosidad\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"‚úÖ SparkSession creada correctamente\")\n",
        "print(f\"Spark Version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. EXTRACCI√ìN (E) - Carga del Dataset\n",
        "\n",
        "Cargamos el dataset limpio generado en el flujo de Pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset limpio\n",
        "data_path = \"../data/gym_lifters_clean.csv\"\n",
        "\n",
        "# Leer CSV con Spark\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(data_path)\n",
        "\n",
        "print(\"‚úÖ Dataset cargado correctamente\")\n",
        "print(f\"Filas: {df.count()} | Columnas: {len(df.columns)}\")\n",
        "print(\"\\nEsquema del DataFrame:\")\n",
        "df.printSchema()\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TRANSFORMACIONES CON SPARK\n",
        "\n",
        "Aplicamos al menos 3 transformaciones diferentes:\n",
        "1. Selecci√≥n y filtrado de columnas\n",
        "2. Agregaciones y c√°lculos\n",
        "3. Creaci√≥n de nuevas columnas derivadas\n",
        "4. Joins (si es necesario)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRANSFORMACI√ìN 1: Selecci√≥n y filtrado\n",
        "# Filtrar registros v√°lidos (con total_kg > 0 y a√±o v√°lido)\n",
        "df_filtered = df.filter(\n",
        "    (col(\"total_kg\").isNotNull()) & \n",
        "    (col(\"total_kg\") > 0) &\n",
        "    (col(\"year\").isNotNull()) &\n",
        "    (col(\"year\") >= 2010) &\n",
        "    (col(\"year\") <= 2025)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Transformaci√≥n 1: Filtrado aplicado\")\n",
        "print(f\"Filas despu√©s del filtrado: {df_filtered.count()}\")\n",
        "df_filtered.select(\"name\", \"country\", \"year\", \"total_kg\", \"competition\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRANSFORMACI√ìN 2: Creaci√≥n de nuevas columnas derivadas\n",
        "# Calcular ratio de eficiencia (total_kg / body_weight_kg)\n",
        "# Calcular diferencia entre clean_and_jerk y snatch\n",
        "df_with_metrics = df_filtered.withColumn(\n",
        "    \"efficiency_ratio\",\n",
        "    when(col(\"body_weight_kg\") > 0, col(\"total_kg\") / col(\"body_weight_kg\")).otherwise(None)\n",
        ").withColumn(\n",
        "    \"lift_difference\",\n",
        "    col(\"clean_and_jerk_kg\") - col(\"snatch_kg\")\n",
        ").withColumn(\n",
        "    \"performance_category\",\n",
        "    when(col(\"total_kg\") >= 350, \"Elite\")\n",
        "    .when(col(\"total_kg\") >= 300, \"Advanced\")\n",
        "    .when(col(\"total_kg\") >= 250, \"Intermediate\")\n",
        "    .otherwise(\"Beginner\")\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Transformaci√≥n 2: Nuevas columnas creadas\")\n",
        "df_with_metrics.select(\n",
        "    \"name\", \"total_kg\", \"body_weight_kg\", \n",
        "    \"efficiency_ratio\", \"lift_difference\", \"performance_category\"\n",
        ").show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRANSFORMACI√ìN 3: Agregaciones por pa√≠s y categor√≠a\n",
        "agg_by_country = df_with_metrics.groupBy(\"country\", \"category\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_lifters\"),\n",
        "        avg(\"total_kg\").alias(\"avg_total_kg\"),\n",
        "        max(\"total_kg\").alias(\"max_total_kg\"),\n",
        "        avg(\"efficiency_ratio\").alias(\"avg_efficiency\")\n",
        "    ) \\\n",
        "    .orderBy(desc(\"avg_total_kg\"))\n",
        "\n",
        "print(\"‚úÖ Transformaci√≥n 3: Agregaciones por pa√≠s y categor√≠a\")\n",
        "agg_by_country.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PROCESO ETL COMPLETO\n",
        "\n",
        "### 3.1 Preparaci√≥n de Tablas Dimensionales\n",
        "\n",
        "Creamos las tablas de dimensiones:\n",
        "- **dim_athlete**: Informaci√≥n de los atletas\n",
        "- **dim_competition**: Informaci√≥n de las competencias\n",
        "- **dim_team**: Informaci√≥n de equipos y coaches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIMENSI√ìN 1: dim_athlete\n",
        "# Crear tabla de atletas √∫nicos con IDs\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Seleccionar atletas √∫nicos (usando name y country como clave de unicidad)\n",
        "dim_athlete = df_with_metrics.select(\n",
        "    \"athlete_id\",\n",
        "    \"name\",\n",
        "    \"gender\",\n",
        "    \"age\",\n",
        "    \"country\"\n",
        ").distinct().filter(\n",
        "    col(\"name\").isNotNull() & (col(\"name\") != \"\")\n",
        ")\n",
        "\n",
        "# Limpiar athlete_id: si est√° vac√≠o o es null, generar uno nuevo\n",
        "dim_athlete = dim_athlete.withColumn(\n",
        "    \"athlete_id_clean\",\n",
        "    when((col(\"athlete_id\").isNull()) | (col(\"athlete_id\") == \"\"), None)\n",
        "    .otherwise(col(\"athlete_id\"))\n",
        ")\n",
        "\n",
        "# Agregar ID num√©rico para la dimensi√≥n (clave primaria)\n",
        "dim_athlete = dim_athlete.withColumn(\n",
        "    \"id_athlete\",\n",
        "    row_number().over(Window.orderBy(\"athlete_id_clean\", \"name\", \"country\"))\n",
        ")\n",
        "\n",
        "# Si athlete_id est√° vac√≠o, generar uno basado en el id num√©rico\n",
        "dim_athlete = dim_athlete.withColumn(\n",
        "    \"final_athlete_id\",\n",
        "    coalesce(\n",
        "        col(\"athlete_id_clean\"), \n",
        "        concat(lit(\"ath_\"), col(\"id_athlete\"))\n",
        "    )\n",
        ").select(\n",
        "    \"id_athlete\",\n",
        "    col(\"final_athlete_id\").alias(\"athlete_id\"),\n",
        "    \"name\",\n",
        "    \"gender\",\n",
        "    \"age\",\n",
        "    \"country\"\n",
        ").distinct()\n",
        "\n",
        "print(\"‚úÖ Dimensi√≥n dim_athlete creada\")\n",
        "print(f\"Total de atletas √∫nicos: {dim_athlete.count()}\")\n",
        "dim_athlete.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIMENSI√ìN 2: dim_competition\n",
        "# Crear tabla de competencias √∫nicas\n",
        "dim_competition = df_with_metrics.select(\n",
        "    \"competition\",\n",
        "    \"year\",\n",
        "    \"category\"\n",
        ").distinct().filter(\n",
        "    col(\"competition\").isNotNull() & \n",
        "    col(\"year\").isNotNull()\n",
        ")\n",
        "\n",
        "# Agregar ID num√©rico\n",
        "dim_competition = dim_competition.withColumn(\n",
        "    \"id_competition\",\n",
        "    row_number().over(Window.orderBy(\"year\", \"competition\", \"category\"))\n",
        ").select(\n",
        "    \"id_competition\",\n",
        "    \"competition\",\n",
        "    \"year\",\n",
        "    \"category\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dimensi√≥n dim_competition creada\")\n",
        "print(f\"Total de competencias √∫nicas: {dim_competition.count()}\")\n",
        "dim_competition.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIMENSI√ìN 3: dim_team\n",
        "# Crear tabla de equipos y coaches √∫nicos\n",
        "dim_team = df_with_metrics.select(\n",
        "    \"team\",\n",
        "    \"coach\"\n",
        ").distinct().filter(col(\"team\").isNotNull())\n",
        "\n",
        "# Agregar ID num√©rico\n",
        "dim_team = dim_team.withColumn(\n",
        "    \"id_team\",\n",
        "    row_number().over(Window.orderBy(\"team\", \"coach\"))\n",
        ").select(\n",
        "    \"id_team\",\n",
        "    \"team\",\n",
        "    \"coach\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dimensi√≥n dim_team creada\")\n",
        "print(f\"Total de equipos √∫nicos: {dim_team.count()}\")\n",
        "dim_team.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Creaci√≥n de Tabla de Hechos\n",
        "\n",
        "Creamos la tabla de hechos que relaciona todas las dimensiones con las m√©tricas de levantamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TABLA DE HECHOS: fact_lifting\n",
        "# Preparar datos para la tabla de hechos con joins a las dimensiones\n",
        "\n",
        "# Preparar el DataFrame base con las claves de uni√≥n\n",
        "fact_base = df_with_metrics.select(\n",
        "    \"athlete_id\",\n",
        "    \"name\",\n",
        "    \"country\",\n",
        "    \"competition\",\n",
        "    \"year\",\n",
        "    \"category\",\n",
        "    \"team\",\n",
        "    \"coach\",\n",
        "    \"snatch_kg\",\n",
        "    \"clean_and_jerk_kg\",\n",
        "    \"total_kg\",\n",
        "    \"body_weight_kg\",\n",
        "    \"event_rank\",\n",
        "    \"medal\",\n",
        "    \"record_status\",\n",
        "    \"lifting_style\",\n",
        "    \"efficiency_ratio\",\n",
        "    \"lift_difference\",\n",
        "    \"performance_category\"\n",
        ")\n",
        "\n",
        "# Join con dim_athlete: primero intentar por athlete_id, luego por name+country\n",
        "fact_with_athlete = fact_base.join(\n",
        "    dim_athlete,\n",
        "    ((fact_base.athlete_id == dim_athlete.athlete_id) & \n",
        "     fact_base.athlete_id.isNotNull() & \n",
        "     (fact_base.athlete_id != \"\")) |\n",
        "    ((fact_base.name == dim_athlete.name) & \n",
        "     (fact_base.country == dim_athlete.country)),\n",
        "    \"inner\"  # Usar inner join para asegurar que todos tengan id_athlete\n",
        ").select(\n",
        "    fact_base[\"competition\"],\n",
        "    fact_base[\"year\"],\n",
        "    fact_base[\"category\"],\n",
        "    fact_base[\"team\"],\n",
        "    fact_base[\"coach\"],\n",
        "    fact_base[\"snatch_kg\"],\n",
        "    fact_base[\"clean_and_jerk_kg\"],\n",
        "    fact_base[\"total_kg\"],\n",
        "    fact_base[\"body_weight_kg\"],\n",
        "    fact_base[\"event_rank\"],\n",
        "    fact_base[\"medal\"],\n",
        "    fact_base[\"record_status\"],\n",
        "    fact_base[\"lifting_style\"],\n",
        "    fact_base[\"efficiency_ratio\"],\n",
        "    fact_base[\"lift_difference\"],\n",
        "    fact_base[\"performance_category\"],\n",
        "    dim_athlete[\"id_athlete\"]\n",
        ")\n",
        "\n",
        "# Join con dim_competition\n",
        "fact_with_competition = fact_with_athlete.join(\n",
        "    dim_competition,\n",
        "    (fact_with_athlete.competition == dim_competition.competition) &\n",
        "    (fact_with_athlete.year == dim_competition.year) &\n",
        "    (coalesce(fact_with_athlete.category, lit(\"\")) == coalesce(dim_competition.category, lit(\"\"))),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    fact_with_athlete[\"id_athlete\"],\n",
        "    fact_with_athlete[\"team\"],\n",
        "    fact_with_athlete[\"coach\"],\n",
        "    fact_with_athlete[\"snatch_kg\"],\n",
        "    fact_with_athlete[\"clean_and_jerk_kg\"],\n",
        "    fact_with_athlete[\"total_kg\"],\n",
        "    fact_with_athlete[\"body_weight_kg\"],\n",
        "    fact_with_athlete[\"event_rank\"],\n",
        "    fact_with_athlete[\"medal\"],\n",
        "    fact_with_athlete[\"record_status\"],\n",
        "    fact_with_athlete[\"lifting_style\"],\n",
        "    fact_with_athlete[\"efficiency_ratio\"],\n",
        "    fact_with_athlete[\"lift_difference\"],\n",
        "    fact_with_athlete[\"performance_category\"],\n",
        "    dim_competition[\"id_competition\"]\n",
        ")\n",
        "\n",
        "# Join con dim_team\n",
        "fact_lifting = fact_with_competition.join(\n",
        "    dim_team,\n",
        "    (fact_with_competition.team == dim_team.team) &\n",
        "    (coalesce(fact_with_competition.coach, lit(\"\")) == coalesce(dim_team.coach, lit(\"\"))),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    col(\"id_athlete\").alias(\"id_athlete\"),\n",
        "    col(\"id_competition\").alias(\"id_competition\"),\n",
        "    col(\"id_team\").alias(\"id_team\"),\n",
        "    \"snatch_kg\",\n",
        "    \"clean_and_jerk_kg\",\n",
        "    \"total_kg\",\n",
        "    \"body_weight_kg\",\n",
        "    \"event_rank\",\n",
        "    \"medal\",\n",
        "    \"record_status\",\n",
        "    \"lifting_style\",\n",
        "    \"efficiency_ratio\",\n",
        "    \"lift_difference\",\n",
        "    \"performance_category\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Tabla de hechos fact_lifting creada\")\n",
        "print(f\"Total de registros en fact_lifting: {fact_lifting.count()}\")\n",
        "fact_lifting.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librer√≠as para SQLite\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Crear directorio warehouse si no existe\n",
        "os.makedirs(\"../warehouse\", exist_ok=True)\n",
        "\n",
        "# Ruta de la base de datos\n",
        "db_path = \"../warehouse/warehouse_pyspark.db\"\n",
        "\n",
        "# Eliminar base de datos existente si existe (para recrearla)\n",
        "if os.path.exists(db_path):\n",
        "    os.remove(db_path)\n",
        "    print(\"‚ö†Ô∏è Base de datos existente eliminada\")\n",
        "\n",
        "# Crear conexi√≥n a SQLite\n",
        "conn = sqlite3.connect(db_path)\n",
        "print(f\"‚úÖ Conexi√≥n a SQLite establecida: {db_path}\")\n",
        "\n",
        "# Convertir DataFrames de Spark a Pandas y cargar en SQLite\n",
        "print(\"\\nüìä Cargando tablas en SQLite...\")\n",
        "\n",
        "# Cargar dim_athlete\n",
        "dim_athlete_pd = dim_athlete.toPandas()\n",
        "dim_athlete_pd.to_sql(\"dim_athlete\", conn, if_exists=\"replace\", index=False)\n",
        "print(f\"‚úÖ dim_athlete: {len(dim_athlete_pd)} registros cargados\")\n",
        "\n",
        "# Cargar dim_competition\n",
        "dim_competition_pd = dim_competition.toPandas()\n",
        "dim_competition_pd.to_sql(\"dim_competition\", conn, if_exists=\"replace\", index=False)\n",
        "print(f\"‚úÖ dim_competition: {len(dim_competition_pd)} registros cargados\")\n",
        "\n",
        "# Cargar dim_team\n",
        "dim_team_pd = dim_team.toPandas()\n",
        "dim_team_pd.to_sql(\"dim_team\", conn, if_exists=\"replace\", index=False)\n",
        "print(f\"‚úÖ dim_team: {len(dim_team_pd)} registros cargados\")\n",
        "\n",
        "# Cargar fact_lifting\n",
        "fact_lifting_pd = fact_lifting.toPandas()\n",
        "fact_lifting_pd.to_sql(\"fact_lifting\", conn, if_exists=\"replace\", index=False)\n",
        "print(f\"‚úÖ fact_lifting: {len(fact_lifting_pd)} registros cargados\")\n",
        "\n",
        "# Cerrar conexi√≥n\n",
        "conn.close()\n",
        "print(f\"\\n‚úÖ Proceso ETL completado. Base de datos guardada en: {db_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Verificaci√≥n de Datos Cargados\n",
        "\n",
        "Verificamos que los datos se hayan cargado correctamente en SQLite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar datos cargados\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Consultar n√∫mero de registros en cada tabla\n",
        "tables = [\"dim_athlete\", \"dim_competition\", \"dim_team\", \"fact_lifting\"]\n",
        "print(\"üìä Resumen de tablas en warehouse_pyspark.db:\\n\")\n",
        "\n",
        "for table in tables:\n",
        "    count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn)\n",
        "    print(f\"{table}: {count['count'].iloc[0]} registros\")\n",
        "\n",
        "# Mostrar muestras de cada tabla\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MUESTRA DE dim_athlete:\")\n",
        "print(\"=\"*50)\n",
        "pd.read_sql_query(\"SELECT * FROM dim_athlete LIMIT 5\", conn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MUESTRA DE dim_competition:\")\n",
        "print(\"=\"*50)\n",
        "pd.read_sql_query(\"SELECT * FROM dim_competition LIMIT 5\", conn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MUESTRA DE dim_team:\")\n",
        "print(\"=\"*50)\n",
        "pd.read_sql_query(\"SELECT * FROM dim_team LIMIT 5\", conn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MUESTRA DE fact_lifting:\")\n",
        "print(\"=\"*50)\n",
        "pd.read_sql_query(\"SELECT * FROM fact_lifting LIMIT 5\", conn)\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Consultas de Ejemplo\n",
        "\n",
        "Ejemplos de consultas que se pueden realizar sobre el Data Warehouse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplos de consultas SQL\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CONSULTA 1: Top 10 atletas por total_kg\")\n",
        "print(\"=\"*60)\n",
        "query1 = \"\"\"\n",
        "SELECT \n",
        "    a.name,\n",
        "    a.country,\n",
        "    a.gender,\n",
        "    COUNT(f.id_athlete) as total_competitions,\n",
        "    AVG(f.total_kg) as avg_total_kg,\n",
        "    MAX(f.total_kg) as max_total_kg\n",
        "FROM fact_lifting f\n",
        "JOIN dim_athlete a ON f.id_athlete = a.id_athlete\n",
        "GROUP BY a.id_athlete, a.name, a.country, a.gender\n",
        "ORDER BY max_total_kg DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result1 = pd.read_sql_query(query1, conn)\n",
        "print(result1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONSULTA 2: Promedio de total_kg por pa√≠s\")\n",
        "print(\"=\"*60)\n",
        "query2 = \"\"\"\n",
        "SELECT \n",
        "    a.country,\n",
        "    COUNT(DISTINCT a.id_athlete) as num_athletes,\n",
        "    AVG(f.total_kg) as avg_total_kg,\n",
        "    MAX(f.total_kg) as max_total_kg\n",
        "FROM fact_lifting f\n",
        "JOIN dim_athlete a ON f.id_athlete = a.id_athlete\n",
        "GROUP BY a.country\n",
        "ORDER BY avg_total_kg DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result2 = pd.read_sql_query(query2, conn)\n",
        "print(result2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONSULTA 3: Competencias con m√°s participantes\")\n",
        "print(\"=\"*60)\n",
        "query3 = \"\"\"\n",
        "SELECT \n",
        "    c.competition,\n",
        "    c.year,\n",
        "    c.category,\n",
        "    COUNT(f.id_athlete) as num_participants,\n",
        "    AVG(f.total_kg) as avg_total_kg\n",
        "FROM fact_lifting f\n",
        "JOIN dim_competition c ON f.id_competition = c.id_competition\n",
        "GROUP BY c.id_competition, c.competition, c.year, c.category\n",
        "ORDER BY num_participants DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result3 = pd.read_sql_query(query3, conn)\n",
        "print(result3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONSULTA 4: Equipos con mejor rendimiento\")\n",
        "print(\"=\"*60)\n",
        "query4 = \"\"\"\n",
        "SELECT \n",
        "    t.team,\n",
        "    t.coach,\n",
        "    COUNT(DISTINCT f.id_athlete) as num_athletes,\n",
        "    AVG(f.total_kg) as avg_total_kg,\n",
        "    COUNT(CASE WHEN f.medal IS NOT NULL AND f.medal != '' THEN 1 END) as total_medals\n",
        "FROM fact_lifting f\n",
        "JOIN dim_team t ON f.id_team = t.id_team\n",
        "GROUP BY t.id_team, t.team, t.coach\n",
        "ORDER BY avg_total_kg DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result4 = pd.read_sql_query(query4, conn)\n",
        "print(result4)\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cerrar SparkSession\n",
        "spark.stop()\n",
        "print(\"‚úÖ SparkSession cerrada correctamente\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
